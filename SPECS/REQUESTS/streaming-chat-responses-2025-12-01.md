# Feature Request: Streaming Chat Responses

**Date:** 2025-12-01
**Status:** Refined

## Overview
Enable real-time streaming of chatbot responses in the web UI using Server-Sent Events (SSE). Tokens are streamed as they're generated by the LLM, providing immediate visual feedback to users.

## Problem Statement
Currently, users must wait for the entire response to be generated before seeing any output. For RAG responses that involve LLM generation (1-5+ seconds), this creates a poor user experience with no feedback during processing.

## Users & Stakeholders
- Primary Users: Web UI users interacting with the chatbot
- Permissions: None required (public access)

## Functional Requirements
1. Stream LLM-generated tokens in real-time via SSE for `information` response types
2. Send metadata (sources, processing_time, session_id) after streaming completes
3. Keep `location_search` responses as instant non-streamed responses (template-based, no LLM)
4. Block user input while streaming is in progress
5. Add response to conversational history only after streaming completes

## User Flow
1. User types question and submits
2. Input field becomes disabled (blocked)
3. Assistant message bubble appears with streaming indicator
4. Tokens stream into the message bubble in real-time
5. After final token, metadata (sources) appears below the response
6. Input field becomes enabled
7. User can type next question

## Acceptance Criteria
- [ ] Backend exposes SSE endpoint for streaming chat responses
- [ ] Frontend establishes SSE connection and renders tokens as they arrive
- [ ] User input is disabled during streaming
- [ ] Sources and metadata display after streaming completes
- [ ] `location_search` responses return immediately without streaming
- [ ] Conversational mode history updated only after stream completes
- [ ] Partial response + error message shown if stream fails mid-response

## User Experience
- **Interface**: Web UI (Next.js frontend)
- **Key Interactions**:
  - Tokens appear progressively in the chat bubble
  - Input field disabled with visual indicator during streaming
- **Feedback**:
  - Streaming indicator before/during token flow
  - Error message appended to partial response on failure

## Technical Requirements
- **Protocol**: Server-Sent Events (SSE)
- **Granularity**: Per-token streaming (stream each token as it arrives)
- **Backend**: FastAPI SSE endpoint (new or modified `/api/chat` endpoint)
- **Frontend**: EventSource API or SSE client library
- **Integration**:
  - LangGraph generator node must yield tokens
  - GROQ/OpenAI streaming API integration
- **Performance**:
  - Minimal latency between token generation and client receipt
  - No buffering/chunking of tokens

## Data Model
- **Storage**: No change - same response data, just delivered incrementally
- **Retention**: N/A
- **Privacy**: N/A

## Edge Cases & Error Handling
1. **Network drop mid-stream** → Show partial response + error message ("Response interrupted. Please try again.")
2. **LLM error mid-stream** → Show partial response + error message
3. **User refreshes page during stream** → Stream terminates, no side effects
4. **Empty response from LLM** → Show appropriate "No response generated" message
5. **Very long response** → Continue streaming, no truncation (scroll follows content)

## Dependencies
- **Requires**:
  - LLM provider (GROQ/OpenAI) streaming API support
  - LangGraph streaming capability in generator node
- **Blocks**: None

## Out of Scope
- WebSocket implementation (SSE chosen for simplicity)
- Cancel/stop button for in-progress streams (can be added later)
- Token-level retry on failure
- Streaming for `location_search` responses (template-based, instant)

## Success Metrics
- Time-to-first-token < 500ms after request
- User perceives immediate response feedback
- No increase in error rate compared to non-streaming

## Notes
- SSE is unidirectional (server → client) which is sufficient for this use case
- Future enhancement: Add stop/cancel button using a separate abort endpoint
- The existing `/api/chat` endpoint can be extended or a new `/api/chat/stream` endpoint created
