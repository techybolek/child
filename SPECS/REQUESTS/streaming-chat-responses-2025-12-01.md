# Feature Request: Streaming Chat Responses

**Date:** 2025-12-01
**Status:** Approved

## Overview
Enable real-time streaming of chatbot responses in the web UI using Server-Sent Events (SSE). Tokens are streamed as they're generated by the LLM, providing immediate visual feedback to users.

## Problem Statement
Currently, users must wait for the entire response to be generated before seeing any output. For RAG responses that involve LLM generation (1-5+ seconds), this creates a poor user experience with no feedback during processing.

## Users & Stakeholders
- Primary Users: Web UI users interacting with the chatbot
- Permissions: None required (public access)

## Functional Requirements
1. Stream LLM-generated tokens in real-time via SSE for `information` response types
2. Send metadata (sources, processing_time, session_id) after streaming completes
3. Keep `location_search` responses as instant non-streamed responses (template-based, no LLM)
4. Block user input while streaming is in progress
5. Add response to conversational history only after streaming completes

## User Flow
1. User types question and submits
2. Input field becomes disabled (blocked)
3. Assistant message bubble appears with streaming indicator
4. Tokens stream into the message bubble in real-time
5. After final token, metadata (sources) appears below the response
6. Input field becomes enabled
7. User can type next question

## Acceptance Criteria
- [ ] Backend exposes SSE endpoint for streaming chat responses
- [ ] Frontend establishes SSE connection and renders tokens as they arrive
- [ ] User input is disabled during streaming
- [ ] Sources and metadata display after streaming completes
- [ ] `location_search` responses return immediately without streaming
- [ ] Conversational mode history updated only after stream completes
- [ ] Partial response + error message shown if stream fails mid-response

## User Experience
- **Interface**: Web UI (Next.js frontend)
- **Key Interactions**:
  - Tokens appear progressively in the chat bubble
  - Input field disabled with visual indicator during streaming
- **Feedback**:
  - Streaming indicator before/during token flow
  - Error message appended to partial response on failure

## Technical Requirements

### Endpoint Design
- **New endpoint**: `POST /api/chat/stream` (separate from existing `/api/chat`)
- **Existing endpoint**: `POST /api/chat` remains unchanged (non-streaming)
- **Frontend toggle**: User can enable/disable streaming in UI settings

### Protocol & Format
- **Protocol**: Server-Sent Events (SSE)
- **Content-Type**: `text/event-stream`
- **Granularity**: Per-token streaming (stream each token as it arrives)

### SSE Event Format
```
event: token
data: {"content": "The"}

event: token
data: {"content": " child"}

event: done
data: {"answer": "...", "sources": [...], "response_type": "information", "session_id": "...", "processing_time": 1.23}

event: error
data: {"message": "Response interrupted", "partial": true}
```

### Request Model (same as `/api/chat`)
```json
{
  "question": "string",
  "session_id": "string (optional)",
  "conversational_mode": "boolean (optional)",
  "llm_model": "string (optional)",
  "reranker_model": "string (optional)",
  "intent_model": "string (optional)",
  "provider": "string (optional)",
  "retrieval_mode": "string (optional)"
}
```

### History Management
- Server accumulates tokens internally as they stream
- After final token, complete response is added to conversational history
- Frontend does not need to send response back

### Location Search Handling
- Streaming endpoint detects `location_search` intent
- Sends template response immediately as single `done` event (no token streaming needed)

### Backend Components
- **`ResponseGenerator.generate_stream()`**: New method using `stream=True` on LLM client
- **`TexasChildcareChatbot.ask_stream()`**: New generator method yielding `(event_type, data)` tuples
- **`/api/chat/stream` route**: FastAPI `StreamingResponse` with SSE formatting
- **Streaming flow (stateless)**: Call classify → retrieve → rerank nodes directly, then stream generation
- **Streaming flow (conversational)**: Run graph nodes up to rerank with checkpointer, stream generation, then finalize history by adding AIMessage to state

### Frontend Components
- **Streaming toggle**: Checkbox/switch in UI to enable/disable streaming
- **EventSource or fetch API**: Handle SSE stream and progressive rendering
- **State management**: Track streaming status, accumulate tokens, display sources on completion

### Performance
- Minimal latency between token generation and client receipt
- No buffering/chunking of tokens

## Data Model
- **Storage**: No change - same response data, just delivered incrementally
- **Retention**: N/A
- **Privacy**: N/A

## Edge Cases & Error Handling
1. **Network drop mid-stream** → Show partial response + error message ("Response interrupted. Please try again.")
2. **LLM error mid-stream** → Show partial response + error message
3. **User refreshes page during stream** → Stream terminates, no side effects
4. **Empty response from LLM** → Show appropriate "No response generated" message
5. **Very long response** → Continue streaming, no truncation (scroll follows content)

## Dependencies
- **Requires**:
  - LLM provider (GROQ/OpenAI) streaming API support (both already support `stream=True`)
- **Blocks**: None

## Out of Scope
- WebSocket implementation (SSE chosen for simplicity)
- Cancel/stop button for in-progress streams (can be added later)
- Token-level retry on failure
- Streaming for `location_search` responses (template-based, instant)

## Success Metrics
- Time-to-first-token < 500ms after request
- User perceives immediate response feedback
- No increase in error rate compared to non-streaming

## Implementation Notes

### Streaming with Conversational Mode
For conversational mode, we must interact with the LangGraph checkpointer to preserve history. The approach is:
1. Run graph nodes up to (but not including) generate: reformulate → classify → retrieve → rerank
2. Stream generation separately via `generate_stream()`
3. After streaming completes, manually add AIMessage to conversation history

This preserves query reformulation (e.g., "What about infants?" resolves correctly) while enabling token streaming.

### Stateless Mode
Simpler flow - call nodes directly without checkpointer involvement, then stream generation.

### SSE vs WebSocket
- SSE is unidirectional (server → client) which is sufficient for this use case
- Simpler to implement than WebSocket
- Native browser support via `EventSource` API

### Future Enhancements
- Add stop/cancel button using a separate abort endpoint
- Migrate to LangGraph native streaming if needed for more complex flows
